Slutsatser kring avvaktiverat hårdvarustöd:

* Prestandavinster ökar. Från två storleksordningars reducering i
  frame time för Julia till 3 storleksordningar. Även erhållna
  prestandavinster för Chess, med en storleksordnings vinst.

* För vissa laster (Julia) kan vi notera att renderingstiden ej
  ökar kraftigt. För Julia 450 ökade endast renderingstiden från
  51 ms till 66 ms. Detta påvisar att prestandavinsten kvarstår.

* Vi kan notera att paravirtualiserad accelerering skalar
  bättre. Paravirtualiserad renderingstid ökade inte med någon
  storleksordning för Julia-benchmark, medan mjukvarurastrerad
  renderingstid ökade med två storeksordningar med hårdvarustöd
  avstängt. På motsvarande vis ökade paravirtualiserad
  renderingstid för Chess-benchmark endast med en
  storleksordning, medan mjukvarurastrering ökade med två
  storleksordningar. Det är inte så viktigt att påpeka hur mycket
  bättre lösningen skalar. Troligtvis är det implicit efter vi
  har presenterat våra prestandamätningar.

* Kostnaden för magiska instruktioner relativt den totala
  renderingstiden har minskat, trots att kostnaden för en magisk
  instruktion har ökat.

* Chess erhar enbart så bra mätvärden tack vare en mycket snabb
  simulator tack vare hårdvaru-virtualiseringsstöd. När vi inte
  har tillgång till detta, t.e.x. vid exekvering på en PPC, så
  erhar vår paravirtualiseringsmetod stora vinster eftersom
  varken JIT-kompilering eller interpretering kan uppnå samma
  hastighet.

Frågor gällande MASCOTS:

* Sedan tidigare modifierar vi resultat efter
  profileringsovergead (subtraherar alla resultat med drygt
  1.5). Bör vi fortsätta med detta eller sluta med det?
  Det spelar ingen roll - det som är lättast.

* Poängtera hur stor prestandavinsten är i storleksordningar.

* Jag hade tänkt ändra namn på våra benchmarks. Bra ide? Är
  Latency och Bandwidth bra namn för benchmarks?
  Nej, bibehåll Julia och Chess. Det är bra namn.

* Behöver vi motivera att vi har egna benchmarks? Ursprungligen
  var motivationen att inget befintligt benchmark hade X och
  Android-stöd. Förslag?
  Nej, det behöver vi inte.

* På flera ställen i artikeln, där data är för lite för att
  presenteras i tabeller, så presenteras detta i
  sprödtext. Exempelvis för magiska instruktioner och
  profileringsoverhead Behöver vi presentera min, max, och
  std-värden för dessa eller räcker genomsnitt? Nej, avg räcker.

* Vissa av våra dataset har _extrema_ avvikelser som långt ökar
  medelvärdet. Hur hanterar man sådana punkter? Kan man bli av
  med dem?
  Ta bort outliers.

* Under beskrivning av respektive benchmark specifieras hur lång
  genomsnittlig tid benchmarket tog att rendera på
  host-systemet. Behöver vi specifiera detta alls eller räcker
  'drygt 16ms'?  Det räcker att poängtera det vi strävade
  efter. Formulera det som 'vi strävade efter 10-20 ms eftersom
  16 ms medför 60fps' (förutsatt att benchmarken renderade mellan
  de tiderna).

* Jag tog bort en referens till en artikel under Previous Work
  som vi inte utvecklade något om. I stil med 'Also related is
  the work by X et al.'. Det finns en till liknande referens som
  inte heller utvecklas om. Bör jag ta bort den också, eller är
  det praktis att man måste nämna X antal arbeten under previous
  work - oavsett om man inte utvecklar kring dem?
  Nej, det är bra att ha kvar dessa. Lägg tillbaka Hansen-referensen.

* Under Previous Work nämns att VMWare utvecklat prototyp för
  grafikaccelerering i en av deras simulatorer. Det kanske inte
  är speciellt relevant för vårt jobb, men det kanske stärker
  relevansen av sådan grafikaccelerering i VMs. Är det en bra
  referens att behålla eller kan den tas bort?
  Ta bort.

* Future work (den punkten du la till), är det bra? Förslag på tillägg?
  Bra.

* Referenser: (Khronos, etc.)
  Lägg till som bibliografi.
