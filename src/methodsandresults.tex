% methodsandresults.tex

% Methods and Results
\section{Methods and Results}
\label{sec:methodsandresults}

The implementation constitutes the realization of the paravirtualized technology described in this document.
From the implementation, several concepts and phrases may be recognized and related to the Android Hardware OpenGL ES emulation design overview (see \dvtcmdciteref{technicaldocs:google:2014}).
In accordance to the Android emulator, paravirtualized graphics acceleration is achieved by the means of three overall components; being the Target System Libraries, the Host System Libraries, and the communications channel aptly named the Simics Pipe.
These components, along with elaboration on the methodologies accommodating them, are described in this chapter (see figure \ref{fig:overview} for an overview of the components and their relationships).

% OpenGL ABI Generation
\subsection{OpenGL ABI Generation}
\label{sec:proposedsolutionandimplementation_openglabigeneration}
For the purposes of ensuring scalability of the solution during development, a set of scripts automating the generation of library source code is used to compile the majority of the target - and host system libraries.
As such, a large amount of the OpenGL function definitions encoded and decoded by the components described in sections \ref{sec:proposedsolutionandimplementation_targetsystemlibraries} and \ref{sec:proposedsolutionandimplementation_hostsystemlibraries} are produced by this tool.
The functionality is implemented in a set of 	exttt{Python} scripts that, from a number of specification/configuration files detailing function signatures and argument attributes, generate both headers and source files in 	exttt{C}; thus collocating the target OpenGL and EGL ABIs, along with the corresponding host decoding libraries.
The tool generates all but the methods that need special treatment (due to state saving) and may thus generate methods with return values and inout arguments.
See figure \ref{fig:abigeneration} for an overview of the code generation process.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{img/yedabigeneration.pdf}
  \caption[The ABI code generation process]{Visualization of the ABI code generating process performed to compile the target- and host system libraries.}
  \label{fig:abigeneration}
\end{figure}

\subsection{Target System Libraries}
\label{sec:proposedsolutionandimplementation_targetsystemlibraries}
The target system libraries are compiled from 	exttt{C} and 	exttt{C++}, and comprise implementations of the EGL - and OpenGL APIs.
Due to the tight coupling in-between OpenGL and the platform windowing system, the solution must also accelerate the Khronos EGL API - the interface between OpenGL and the underlying platform windowing system (see section \ref{sec:proposedsolutionandimplementation_windowingsystems} for an elaboration on the full extent of EGL interaction).
As may be derived from the name, the target system libraries run on the simulation target system and replace the existing Khronos libraries.

As such, the target systems libraries implement the EGL - and OpenGL~ES~$2.0$ APIs and lures whatever application it is being linked to that it is, in fact, the expected platform libraries.
Given that the target system libraries adheres to the OpenGL headers defined in the system, the application is na\"{\i}ve in terms of it's paravirtualized status.
The interplay with the original OpenGL~ES headers also results in the solution adhering to the platform-dependent type definition, flags, and constants; as originally defined by Khronos.
However, instead of communicating with the platform windowing system (in terms of EGL ) and the graphics device (in terms of OpenGL ) - and instructing said device in coagency with the user; the target system libraries rather serialize the given command stream and forwards it to the simulation host.
However, the transmission of the command stream is not necessarily performed at once, or in the designated order due to the formation of the OpenGL~ES~$2.0$ framework.
This complex of problems involve uncertainties of the proportion of argument data, as size is not necessarily specified by the user.
As such, certain serialization may have to be delayed until further information surrounding the argument dimensions have been relayed to the OpenGL library.
Furthermore, a subset of the OpenGL state need be maintained by the target system libraries.
These attributes are comprised by, inter alia, bound vertex- and index element buffers, in addition to properties of OpenGL vertex attributes.
Such states must be kept in the target system libraries due to the asynchronous nature of serialization of OpenGL invocations in the paravirtualized solution.

The serialization described in the above paragraph is thus formatted and encoded in accordance to a certain data format, which is kept as minimal as possible throughout execution.
This encoding includes packing variable length data types, such as $8$-bit characters, $16$-bit fields, or $64$-bit integer values, into fixed length structures, so that the host system may interpret these values independently of how corresponding types are defined on that unrelated platform\footnote{It should be noted, however, that the solution assumes a little endian architecture and IEEE 754 standard for floating point representation. If the host system would not conform to these prerequisites, the solution would have to be complemented with additional support.}.

% Host System Libraries
\subsection{Host System Libraries}
\label{sec:proposedsolutionandimplementation_hostsystemlibraries}
In collaboration with the target system libraries, the host system libraries - running on the host system - subsequently decodes and interprets the received byte stream.
Said decoding involves unpacking data from fixed length storage into variable-size types that the OpenGL - and EGL libraries expect.
Furthermore, and again similarly to the target system libraries, due to design inherent in the OpenGL~ES~$2.0$ framework, the host system libraries need maintain some data; in particular - vertex attributes.
Such data is buffered in the host system libraries until drawn in a later, and separate, OpenGL invocation\footnote{A possible optimization would be to cache said data, to avoid the need to transmit unmodified vertices multiple times, despite so being specified by the user.}.
When the requested OpenGL invocation has been performed, any return- or in-out values are returned to the target system using the Simics Pipe (see section \ref{sec:proposedsolutionandimplementation_simicspipe}).
As with the the target system libraries, the receiving end of an OpenGL method definitions in the host system libraries are likewise generated to a large degree.
The host system libraries are written in \texttt{C} and \texttt{C++}.

% Windowing Systems
\subsection{Windowing Systems}
\label{sec:proposedsolutionandimplementation_windowingsystems}
Due to variations in the creation and maintenance of windows on different platforms (i.e., Fedora 19 and Android ), incurred by said platforms utilizing different interfaces for the purpose, the window to which OpenGL renders is kept on the simulation host.
The problem may be summarized as the dilemma of the target system libraries having to communicate with the correct window (located in the simulation host ), yet having the target system window reporting successful initialization.
After all, it would be problematic if the OpenGL -utilizing application would have to be modified in order to be paravirtualized.
Effectively, this means that it would be desirable to maintain the native functionality of the target system EGL library.
However, in order to swap the backbuffers to which OpenGL renders (in the window present in the host system), one must use EGL methods.
In this way, the problem comprises a conflict in-between wanting to keep the functionality of the native EGL library, yet modify a small subset of it. 
This issue is overcome by the use of symbol overriding\footnote{Utilizing \dvtcmdcodeinline{LD_PRELOAD} on the Linux platform.}, which allows the target system libraries to overload a function, serialize and forward the invocation to the host system, locate the next occurrence of the symbol in the symbol table (being the original native EGL function definition), and invoke the original function.
As such, the target system EGL library does not replace the native target EGL library, as with the OpenGL library, but rather overloads some of its definition.
This gives the effect of a successfully created window, not having returned any errors in the window creation and maintenance - yet having the application actually communicating with a different window (present in the host system).
As such, the target system libraries are effectively performing a man-in-the-middle attack on the target EGL library.

In order to run the benchmarks described in section \ref{sec:experimentalmethodology_benchmarks}, the target system libraries need simply overload the \dvtcmdcodeinline{eglSwapBuffers} function, but the solution could easily be extended to listen in on requested window dimensions and other window attributes; effectively circumventing the need to heed any differences in-between platform-specific windowing systems.

% Simics Pipe
\subsection{Simics Pipe}
\label{sec:proposedsolutionandimplementation_simicspipe}
The so named 'Simics Pipe' constitutes the target -to-host communications channel in Simics.
As such, it is responsible for transferring a serialized command stream - or byte stream - to- and from the simulation host using magic instructions.
The role of the Simics Pipe is comprised of the allocation and maintenance of page locked target memory.
Furthermore, the Simics Pipe utilizes a magic instruction to relay the starting address of said memory space to the simulation host; where the responsibilities of the Simics Pipe end.

There are a numerous methodologies to communicate with the outside world from inside of the simulation, such as utilizing UNIX sockets or other networking technologies.
However, due to the inherent performance demands brought on by serialization of real-time graphics invocations, the methodology of magic instructions were selected for use for the purpose of this study.
Magic instructions allow for fast, in the pretext of the simulation target - almost instant - escape from the simulation context, and may carry a limited amount of information with it from inside the simulation out into the real world.
However, other - albeit slower, methodologies may be fast enough to for use in graphics paravirtualization; the Android emulator, in addition to magic instructions, supports TCP/IP communication\footnote{It may be of interest to know that the Simics OpenGL~ES paravirtualization technology, during development, was accommodated by a TCP/IP client-/server model.}.

In order to perform a magic instruction, the Simics Pipe uses \texttt{GCC} Extended Asm to directly inject \dvtcmdcodeinline{__volatile__}-flagged (as to prevent the compiler from reordering memory accesses during optimization) \texttt{assembly} instructions into 	exttt{C} code, in which - in addition to 	exttt{C++} - the Simics Pipe is written.
During said instruction, one may simply write an arbitrary 64-bit address to any register fit for the purpose (the number- and size of processor registers being the data-sharing bottleneck of the magic instruction paradigm).
Not being the first time magic instructions have been used for the purposes of hardware acceleration~\dvtcmdcitebib[p.~32]{publications:leupers:2010}, the Simics Pipe utilizes such methodology, namely the \dvtcmdcodeinline{CPUID} x86 instruction, to carry a lone memory address (being the address of the serialized command stream) in the target \dvtcmdcodeinline{rbx}\footnote{Accordingly, if the simulation target should correspond to a 32-bit x86 system, said register would translate to the \dvtcmdcodeinline{ebx} register.} register.
The execution of the injected instruction in a simulated processor, in line with the magic instruction paradigm, invokes a callback method in the host side of the Simics Pipe; having effectively escaped the simulation and paused the simulation state.
Knowing the occurrence of a magic instruction with the corresponding leaf number, one may assume the existence of a target memory address in the simulated target CPU register \dvtcmdcodeinline{rbx}.
Note that, at this point, the retrieved address is in the format of a target system virtual address; the destination of which is unknown to the simulation host.
As such, this address need be translated in order to retrieve the package contents said memory address points to.
See section \ref{sec:proposedsolutionandimplementation_pagetabletraversal} for more information on how the address is interpreted and how the entirety of the intended sequence of bytes is retrieved from target system primary memory.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/yedoverview.pdf}
\caption[Paravirtualization implementation overview]{Overview of the implementation accomodating paravirtualized graphics in Simics.}
\label{fig:overview}
\end{figure}

% Page Table Traversal
\subsection{Page Table Traversal}
\label{sec:proposedsolutionandimplementation_pagetabletraversal}
As outlined in section \ref{sec:proposedsolutionandimplementation_simicspipe}, target -/host memory sharing in the Simics Pipe is performed by the means of exposing a target virtual address to the simulation host.
Said virtual address is, understandably so, only valid in the simulated system and bears no relevance in the real world; outside of the fact that the data to which the virtual address refers - in the target system - is, in turn, hidden behind layers of abstraction somewhere in host system memory.

Using Simics to utilize the MMU of the target system, one may translate a target virtual address (or 'virtualized virtual address') to a (target ) physical device address; to-/from which - again, using Simics - we may access an arbitrary number of bytes in physical memory.
However, due to the complexity induced by circumventing the abstraction of virtual memory, there is no guarantee that the memory page to which the exposed physical address refers has not been swapped out of primary memory.
In order to solve this, using some OSs (e.g., Fedora 19 \footnote{Using the \dvtcmdcodeinline{MAP_LOCKED} flag during invocation of the Linux \dvtcmdcodeinline{mmap}-system call; alternatively using the specific \dvtcmdcodeinline{mlock}-system call.}), one may 'lock' pages in order to prevent them from being swapped to disk.
This is the methodology used to ensure that the physical memory space, referred to by the virtual address sent to the simulation host, is still existent in target primary memory when the simulation state has been paused\footnote{Other methods to achieve this include repeatedly 'polling' the corresponding memory pages in the target system before inducing the simulation context switch to the host system.}.
Furthermore, and again induced by the unorthodox circumvention of the virtual memory paradigm, it is probable that the multiple memory pages making out particularly large serialized command streams, such as the transmission of vertex or texture data, are not consecutively aligned in physical memory, although guaranteed to be continuous blocks in terms of virtual memory.
As such, the physical addresses of memory pages must be continuously retrieved and translated on a per-page basis; effectively 'traversing' the virtual memory table (see figure \ref{fig:virtualmemory}).
This can be done in a trivial manner by simply iterating the original virtual address with the \textit{target } page size, in our case, $4096$ bytes.
Naturally, said process must be performed regardless of data being read or written to-/from the intended - physical - memory space.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/yedvirtualmemory.pdf}
\caption[Memory translation overview]{Memory translation overview. The OpenGL process hands a virtual memory address, pointing somewhere in the target system \textit{primary} memory, to the paravirtualized solution - which inquiries the target system MMU to retrieve designated bytestream directly from target physical memory.}
\label{fig:virtualmemory}
\end{figure}
% \footnote{Observe that said bytestream may not be present in secondary storage, due to memory page locking.}

\subsection{Experimental Methodology}
\label{sec:experimentalmethodology}
From this chapter onward, the term 'experiment' is used to refer to the experimental methodology put in place to answer the question formulations phrased in chapter \ref{cha:aimsandobjectives}.
The author would like to emphasize that the terminology is simply used to denote a method of experimentation to answer, that is verify or refute the validity of the hypotheses that may be intrinsically derived from-, the research questions outlined in chapter \ref{cha:aimsandobjectives}.
As such, 'experiment' does not refer to controlled-, natural-, or field experiment methodologies, or indeed any other specific method for experimentation unless specified otherwise.
The author reserves the right to use the following terminology interchangeably: study, case study, and experiment.

As such, the terminology used to describe the experimental methodology produced for the purpose of this study, henceforth referred to as 'the experiment', is not meant to be indicative of the method quality, or in any way indirectly describe said experimental methodology.
Below, the methods used to perform the experiment are presented.

% Platform Configuration
\subsubsection{Platform Configuration}
\label{sec:experimentalmethodology_platformconfiguration}
The experiment devised for the purpose of this paper is performed in software rasterized- and paravirtualized Simics platforms, respectively.
Furthermore, the experiment profiles the performance of the Android emulator to relate the devised solution to pre-existing technologies, in addition to execution on the hardware accelerated host platform as a reference case.
The following paragraphs outline the configuration of these target platforms, in terms of host configuration, simulated hardware, and the software configuration of the simulated systems.

% Host Configuration
\paragraph{Host Configuration}
\label{par:experimentalmethodology_platformconfiguration_hostconfiguration}
The system upon which the experiment is performed is an x86 -compatible Haswell Intel\circledR processor configured to run the Fedora 19 Linux distribution.
Said OS was selected for use due to Fedora 19 being the primary platform used for development of the Simics full-system simulator at the Intel\circledR offices in Stockholm - at which the solution described in this document has been developed.

% Target Hardware Configuration
\paragraph{Target Hardware Configuration}
\label{par:experimentalmethodology_platformconfiguration:targethardwareconfiguration}
The Simics hardware configuration utilized for the purpose of this experiment simulates an Intel\circledR\ Core	exttrademark\ i7; the same processor series to that of the actual hardware of the simulation host system.
Furthermore, Simics execution, both the software rasterized and paravirtualized platforms, utilize KVM for the simulation target to run natively on the host hardware.
In order to accommodate for similar acceleration in QEMU, Android is configured to run on the Intel\circledR Android x86 system image (see \dvtcmdciteref{technicaldocs:intel:2013:x86}), circumventing the need to interpret ARM instructions~\dvtcmdciteref{web:stylianou:2010}.
Furthermore, the solution also utilize KVM in order to provide similar native execution on the host hardware\footnote{Had the solution been running on the Windows OS, the solution would have utilized Intel\circledR\ HAXM (see ~\dvtcmdciteref{technicaldocs:intel:2013:haxm}) to accommodate for said native execution~\dvtcmdciteref{web:hofemeier:2010}.}.

% Target Software Configuration
\paragraph{Target Software Configuration}
\label{par:experimentalmethodology_platformconfiguration_targetsoftwareconfiguration}
In line with Fedora $19$ being the OS in use when performing the reference benchmark experiments on the host platform (see paragraph \dvtcmdrefname{par:experimentalmethodology_platformconfiguration_hostconfiguration}), it may be of value to have the simulated target machine configured to run with the same OS.
As such, the platform OSs used for the purpose of this study are as follows:
\begin{itemize}[noitemsep]
\item Hardware accelerated Fedora $19$ host system
\item Software rasterized\footnote{Using the Mesa OpenGL software rasterization implementation.} Fedora 19 Simics target system
\item Paravirtualized Fedora $19$ Simics target system
\item Paravirtualized Android $4.4$\footnote{Android KitKat - API level $19$.} QEMU target system
\end{itemize}

% Benchmarks
\subsubsection{Benchmarks}
\label{sec:experimentalmethodology_benchmarks}
Throughout the course of the pilot study, no existing OpenGL~ES~$2.0$ benchmark (featuring cross-platform profiling support for Android and X11 Linux ) was deemed appropriate for for the purposes of this experiment.
As such, a number of benchmarks have been devised on-site for the purposes of stress-testing the paravirtualized technology described in this document.
The benchmarks, numbering three in total, are intended to stress suspected bottlenecks in the implementation; corresponding to a large number of relatively insignificant OpenGL~ES invocations, computationally intensive GPU kernels, and passing of large data such as textures or models.
Since the benchmarks are required to run in both Linux - and Android platforms, the benchmarking suite utilize Java native interface to invoke OpenGL~ES from the same 	exttt{C} code base independent of platform.
Effectively, this entails having the benchmarks produced using \texttt{Java}, \texttt{C}, and \texttt{GLSL}, collectively.
Furthermore, all benchmarks are configured to run at roughly $16$~\milli\second , which would correspond to roughly $60$~frames per second, when hardware accelerated on the host system.
The benchmarks are devised in this way in order to reflect the expected load of a modern real-time interactive application.
As such, the purpose of developed benchmarks is to be representative of typical scenarios induced by modern graphics applications whilst utilizing a graphics framework such as OpenGL.
When run during the experiment, each benchmark instance measures the elapsed time of $1000$ frames which makes up the data subsequently analyzed.
Frame captures of the benchmarks are presented in figures \ref{fig:benchmarks_chess}, \ref{fig:benchmarks_julia}, and \ref{fig:benchmarks_phong}.

\begin{figure}
  \minipage{0.32\linewidth}
  \includegraphics[width=\linewidth]{img/imgchess.png}
  \caption[Chess benchmark screen capture]{Chess.}
  \label{fig:benchmarks_chess}
  \endminipage\hfill
  \minipage{0.32\linewidth}
  \includegraphics[width=\linewidth]{img/imgjulia.png}
  \caption[Julia benchmark screen capture]{Julia.}
  \label{fig:benchmarks_julia}
  \endminipage\hfill
  \minipage{0.32\linewidth}
  \includegraphics[width=\linewidth]{img/imgphong.png}
  \caption[Phong benchmark screen capture]{Phong.}
  \label{fig:benchmarks_phong}
  \endminipage
\end{figure}

% Benchmark: Chess
\paragraph{Benchmark: Chess}
\label{par:experimentalmethodology_benchmarking_benchmarkchess}
\index{Chess benchmark}
The 'Chess' benchmark is developed for the purposes of stressing the latency in-between target - and host systems.
It is so named because of the chess-like tileset the graphics kernel produces.
The benchmark is designed to perform a multitude of OpenGL~ES~$2.0$ library invocations per frame; in which each invocation is relatively lightweight in execution and carry a small amount of data argument-wise.
In the Chess benchmark, this is achieved by rendering a grid of colored (black or white, in order to adhere to the chess paradigm) rectangles where each tile is represented by four two-dimensional vertices in screen-space, in addition to six indices outlining the rectangular shape.
Since the vertices are already transformed into screen-space, the graphics kernel need perform no additional transformation, adhering to the desired lightweight behavior of each kernel invocation.
Additionally, the tileset vertices and indices are pre-loaded into OpenGL vertex- and index element buffers, so that a lone buffer identifier may be carried over in-place of the heavier vertex set load.
Each tile is then individually drawn to the backbuffer, rendering the chess-like appearance of the benchmark.
Effectively, this means that, for each tile, the benchmark need only bind a vertex- and an index element buffer, set the corresponding tile color, and lastly invoke the rendering of said tile.

For each frame rendered, depending on the number of drawn tiles, the solution will perform a large number of magic instructions.
This induces a high utilization of the Simics Pipe, which is intended to stress magic instruction overhead (section \ref{sec:results_magicinstructionoverhead}).
The repeated invocation of lesser draw calls is representative of common usage of drawing a multitude of shapes with OpenGL, such as a user interface. Additionally, the number of tiles being computed is easily modifiable; rendering the benchmark scalable for the purposes of the experiment described in this document. As such, said benchmark is considered suitable for the purpose of representing a large number of graphics invocations using OpenGL~ES~$2.0$.

% Benchmark: Julia
\paragraph{Benchmark: Julia}
\label{par:experimentalmethodology_benchmarking_benchmarkjulia}
\index{Julia fractal benchmark}
The 'Julia' benchmark is developed for the purposes of stressing computational intensity in software-rasterized and paravirtualized platforms.
It is so named due to the kernel calculating the Julia fractal; the texturing and frame-wise seeding of which gives the benchmark it's distinct look.
The benchmark is designed to perform a lone computationally intensive graphics kernel invocation, which will stress the computational prowess of the profiled platform.
The case is selected for use as the computation of a fractal is trivially scalable in terms of complexity, by modifying the number of iterations the fractal algorithm performs, and is thus considered suitable for profiling of computationally intensive graphics kernels.

% Benchmark: Phong
\paragraph{Benchmark: Phong}
\label{par:experimentalmethodology_benchmarking_benchmarkphong}
\index{Phong shading benchmark}
The 'Phong' benchmark is developed for the purposes of stressing the throughput, or bandwidth - if adhering to the networking paradigm, in-between target - and host systems.
The Phong benchmark is comprised of a rotating model, being the Newell teapot, which is textured and subsequently shaded by a single point light using the Phong shading model; thus giving the benchmark it's name. % \todo{point out as to why the newell teapot is a standard model}

The rasterization and shading of a model with a given, large, texture is representative of three-dimensional graphics commonly rendered with graphics frameworks such as OpenGL~ES~$2.0$.
As such, said benchmark is suitable for the purposes of representing the usage of big data (being models, textures, etc.).
For the purposes of stressing the bandwidth of target - and host communication, the Phong benchmark is easily scalable in terms of resizing the large texture in question.
Vertices and indices of the model do not utilize OpenGL vertex- and-/or index element buffers, thus forcing the solution to transmit the data every frame.
Additionally, in order to further stress the throughput of the profiled platform, texture data is updated every frame.

% Input Data Variation
\subsubsection{Input Data Variation}
\label{sec:experimentalmethodology_inputdatavariation}
In order to detect anything but linear potential of scaling in software rasterized- and paravirtualized Simics platforms, the benchmarks are configured to run in three separate instances, respectively.
These benchmark versions differ in terms of input data; where said input is a changed variable that could potentially worsen benchmark performance (e.g., larger texture).
The purpose of these 'input data variations' is to detect any unexpected results in execution; and thus identify any performance complexity issues in software rasterized- and paravirtualized Simics platforms.
For consistency, each benchmark variation - of which there are two; resulting in three unique experiments per benchmark - have been produced to halve- and subsequently double the corresponding input data.
The described input data, for each benchmark, is presented in table \ref{tab:keyvals}.
Consequently, in table \ref{tab:keyvalsmagicinstructions}, the per-frame number of magic instructions induced by these input data variations are presented, differing only for the Chess benchmark.

% Renders a table defining the key figures used for benchmark
% variations for the purpose of the experimend performed for this
% paper.
\begin{table}
  \centering
  \begin{tabular}{lllll}
    Benchmark & Input Data & Halved Input & Ref. Input & Double Input \\ \hline
    Chess & No. Tiles & $60\times60$ & $84\times84$ & $118\times118$ \\
    Julia & No. Iterations & $225$ & $450$ & $900$ \\
    Phong & Texture Resolution & $1448\times1448$ & $2048\times2048$ & $2896\times2896$ \\
  \end{tabular}
  \caption[Input data variations]{Input data used for benchmark variations for each benchmark.}
  \label{tab:keyvals}
\end{table}

% Renders a table defining the number of magic instructions performed
% for the benchmarks used in this experiment.
\begin{table}
  \centering
  \begin{tabular}{lllll}
    Benchmark & \phantom{Input Data} & Halved Input & Ref. Input & Double Input \\ \hline
    Chess & \phantom{No. Tiles} & $32403$ & $63507$ & $125319$ \\
    Julia & \phantom{No. Iterations} & $16$ & $16$ & $16$ \\
    Phong & \phantom{Texture Resolution} & $17$ & $17$ & $17$ \\
  \end{tabular}
  \caption[Input data variation magic instruction count]{Number of magic instructions performed per-frame for each benchmark input data variation.}
  \label{tab:keyvalsmagicinstructions}
\end{table}

Note that for each tile rendered in the Chess benchmark, the solution will perform $9$ magic instructions; entailing a total of $32400$\footnote{$9\times60\times60=32400$.}, $63504$\footnote{$9\times84\times84=63504$.}, and $125316$\footnote{$9\times118\times118=125316$.} magic instructions, respectively, in addition to a minor number per-frame.

\subsection{Results}
\label{sec:results}
In this chapter, the results collected from having applied the described experiment methodology onto the devised solution are presented.
As such, the results gathered from execution on the host is presented in figure \ref{fig:histogramshost}, the results compiled from execution in the Android emulator presented in figure \ref{fig:histogramsqemu}, and the results accumulated from software rasterized- and paravirtualized execution in Simics are presented in figures \ref{fig:histogramssimicsparachess}, \ref{fig:histogramssimicsparajulia}, and \ref{fig:histogramssimicsparaphong}.
Note that the data compiled from software rasterized- and paravirtualized Simics platforms include additional data from having performed the experiment using the input data variations, as described in section \ref{sec:experimentalmethodology_inputdatavariation}.

In this chapter, the results are compiled into histograms; visualizing elapsed time in milliseconds to sample density.
As such, the $Y$ axis showcase sample density; although the axis keys have been removed as they bear little relevance to the outcomes presented in this material.
The histograms each feature $100$ bins; into which a $1000$ samples, for each experiment performed, are rounded into.
For the purposes of good visualization methodology, values outside of the standard deviation\footnote{That is; values above that of the $mean + std$ and values under that of $mean - std$.} are not featured in the figures presented in this section.
In order to accommodate for the, however few, samples outside of said limits the figures are all complemented with key ratio tables (see tables \ref{tab:keyvalhost}, \ref{tab:keyvalqemu}, \ref{tab:keyvalsimics}, and \ref{tab:keyvalpara}).

% Benchmark Results
\subsubsection{Benchmark Results}
\label{sec:results_benchmarkresults}
Based on the reference profiling presented in figure \ref{fig:histogramshost}, we may conclude that the benchmarks, when hardware accelerated on the host system, perform with concentrated density; not being much scattered across the graph except a few irregular extremities in terms of maximum frame times (see table \ref{tab:keyvalhost}).
This is supported by the standard deviation presented in said table.
Furthermore, we may conclude that the Phong demo features two distinct peaks in density distribution - about $1$ ms in-between.
There may be cause to believe that this is caused, or partly caused, by frame-wise rotation of the teapot - rotation featured in the benchmark (see section \ref{sec:experimentalmethodology_benchmarks}) - inducing some fluctuation into its execution (see figure \ref{fig:histogramssimicsparaphong}).
However, this behavior is, strangely so, not apparent whilst paravirtualized in the QEMU derived Android emulator, although this may be a visual artifact due to the resolution of the graph (see figure \ref{fig:histogramsqemu}).
See section \ref{sec:threatstovalidity_benchmarkvariations} for an elaboration on divergence in the Phong benchmark.
Additionally, and in accordance to tables \ref{tab:keyvalhost} and \ref{tab:keyvalqemu}, one may observe relatively high recorded maximum frame times in relation to compiled maximum- and average values, yet featuring - in relation to divergent maximum, relatively low standard deviations.

The remainder of this chapter will present a benchmark-Wise analysis of the data compiled from executing the experiment on the software rasterized- and paravirtualized Simics platforms; said platforms being the subject of this study.
For the sake of brevity, these analyses are segmented into paragraphs for each benchmark.
These paragraphs are presented below.

\begin{figure}
  \centering
  \input{gnuhistogramshost.tex}
  \caption[Benchmark results - hardware accelerated on the simulation host]{Histogram depicting benchmark elapsed frametimes in milliseconds and the density distribution of 1000 frames whilst hardware accelerated on the simulation host. The $Y$ axis thus depict sample density. Its axis keys have been removed as they bear no relevance to the outcomes presented in this document.}
  \label{fig:histogramshost}
\end{figure}

\begin{figure}
  \centering
  \input{gnuhistogramsqemu.tex}
  \caption[Benchmark results - paravirtualized in QEMU]{Histogram depicting benchmark elapsed frametimes in milliseconds and the density distribution of 1000 frames whilst paravirtualized in QEMU. The $Y$ axis thus depict sample density. Its axis keys have been removed as they bear no relevance to the outcomes presented in this document.}
  \label{fig:histogramsqemu}
\end{figure}

% Defines a table outlining the performance of the paper benchmark
% whilst hardware accelerated on the host platform.
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Benchmark} & \multicolumn{4}{p{6cm}|}{\centering Elapsed time (milliseconds)} \\
\cline{2-5} & \multicolumn{1}{c|}{Min} & \multicolumn{1}{c|}{Max} & \multicolumn{1}{c|}{Std} & \multicolumn{1}{c|}{Avg} \\ \hline
Chess & \dvtcmdfirstline{hostchess84x84.dat.min} & \dvtcmdfirstline{hostchess84x84.dat.max} & \dvtcmdfirstline{hostchess84x84.dat.std} & \dvtcmdfirstline{hostchess84x84.dat.avg} \\ \hline
Julia & \dvtcmdfirstline{hostjulia450.dat.min} & \dvtcmdfirstline{hostjulia450.dat.max}	& \dvtcmdfirstline{hostjulia450.dat.std} & \dvtcmdfirstline{hostjulia450.dat.avg} \\ \hline
Phong & \dvtcmdfirstline{hostphong2048x2048.dat.min} & \dvtcmdfirstline{hostphong2048x2048.dat.max} & \dvtcmdfirstline{hostphong2048x2048.dat.std} & \dvtcmdfirstline{hostphong2048x2048.dat.avg} \\ \hline
\end{tabular}
\caption[Benchmark results - hardware accelerated on the simulation host]{Benchmarking results whilst hardware accelerated on the simulation host system.}
\label{tab:keyvalhost}
\end{table}

% Defines a table outlining the performance of the paper benchmark
% whilst paravirtualized in the QEMU-derived Android Emulator.
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Benchmark} & \multicolumn{4}{p{6cm}|}{\centering Elapsed time (milliseconds)} \\
\cline{2-5} & \multicolumn{1}{c|}{Min} & \multicolumn{1}{c|}{Max} & \multicolumn{1}{c|}{Std} & \multicolumn{1}{c|}{Avg} \\ \hline
Chess & \dvtcmdfirstline{qemuchess84x84.dat.min} & \dvtcmdfirstline{qemuchess84x84.dat.max} & \dvtcmdfirstline{qemuchess84x84.dat.std} & \dvtcmdfirstline{qemuchess84x84.dat.avg} \\ \hline
Julia & \dvtcmdfirstline{qemujulia450.dat.min} & \dvtcmdfirstline{qemujulia450.dat.max}	& \dvtcmdfirstline{qemujulia450.dat.std} & \dvtcmdfirstline{qemujulia450.dat.avg} \\ \hline
Phong & \dvtcmdfirstline{qemuphong2048x2048.dat.min} & \dvtcmdfirstline{qemuphong2048x2048.dat.max} & \dvtcmdfirstline{qemuphong2048x2048.dat.std} & \dvtcmdfirstline{qemuphong2048x2048.dat.avg} \\ \hline
\end{tabular}
\caption[Benchmark results - paravirtualized in the Android emulator]{Benchmarking results whilst paravirtualized in the QEMU-derived Android emulator.}
\label{tab:keyvalqemu}
\end{table}

% Defines a table outlining the performance of the paper benchmark
% whilst software rasterized in the Simics full-system simulator.
\begin{table*}
  \centering
  \providecommand{\chesskeyone}{$60\times60$ tiles}
  \providecommand{\chesskeytwo}{$84\times84$ tiles}
  \providecommand{\chesskeythree}{$118\times118$ tiles}

  \providecommand{\juliakeyone}{$225$ iterations}
  \providecommand{\juliakeytwo}{$450$ iterations}
  \providecommand{\juliakeythree}{$900$ iterations}

  \providecommand{\phongkeyone}{$1448\times1448$ texels}
  \providecommand{\phongkeytwo}{$2048\times2048$ texels}
  \providecommand{\phongkeythree}{$2896\times2896$ texels}

  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Benchmark} & \multirow{2}{*}{Key} & \multicolumn{4}{p{6cm}|}{\centering Elapsed time (milliseconds)} \\
    \cline{3-6} && \multicolumn{1}{c|}{Min} & \multicolumn{1}{c|}{Max} & \multicolumn{1}{c|}{Std} & \multicolumn{1}{c|}{Avg} \\ \hline
    \multirow{3}{*}{Chess} & \chesskeyone & \dvtcmdfirstline{simicschess60x60.dat.min} & \dvtcmdfirstline{simicschess60x60.dat.max}	& \dvtcmdfirstline{simicschess60x60.dat.std} & \dvtcmdfirstline{simicschess60x60.dat.avg} \\ %\cline{2-6}
    & \chesskeytwo & \dvtcmdfirstline{simicschess84x84.dat.min} & \dvtcmdfirstline{simicschess84x84.dat.max} & \dvtcmdfirstline{simicschess84x84.dat.std} & \dvtcmdfirstline{simicschess84x84.dat.avg} \\ %\cline{2-6}
    & \chesskeythree & \dvtcmdfirstline{simicschess118x118.dat.min} & \dvtcmdfirstline{simicschess118x118.dat.max} & \dvtcmdfirstline{simicschess118x118.dat.std} & \dvtcmdfirstline{simicschess118x118.dat.avg} \\ \hline
    \multirow{3}{*}{Julia} & \juliakeyone & \dvtcmdfirstline{simicsjulia225.dat.min} & \dvtcmdfirstline{simicsjulia225.dat.max} & \dvtcmdfirstline{simicsjulia225.dat.std} & \dvtcmdfirstline{simicsjulia225.dat.avg} \\ %\cline{2-6}
    & \juliakeytwo & \dvtcmdfirstline{simicsjulia450.dat.min} & \dvtcmdfirstline{simicsjulia450.dat.max} & \dvtcmdfirstline{simicsjulia450.dat.std} & \dvtcmdfirstline{simicsjulia450.dat.avg} \\ %\cline{2-6}
    & \juliakeythree & \dvtcmdfirstline{simicsjulia900.dat.min} & \dvtcmdfirstline{simicsjulia900.dat.max} & \dvtcmdfirstline{simicsjulia900.dat.std} & \dvtcmdfirstline{simicsjulia900.dat.avg} \\ \hline
    \multirow{3}{*}{Phong} & \phongkeyone & \dvtcmdfirstline{simicsphong1448x1448.dat.min} & \dvtcmdfirstline{simicsphong1448x1448.dat.max}	& \dvtcmdfirstline{simicsphong1448x1448.dat.std} & \dvtcmdfirstline{simicsphong1448x1448.dat.avg} \\ %\cline{2-6}
    & \phongkeytwo & \dvtcmdfirstline{simicsphong2048x2048.dat.min} & \dvtcmdfirstline{simicsphong2048x2048.dat.max} & \dvtcmdfirstline{simicsphong2048x2048.dat.std} & \dvtcmdfirstline{simicsphong2048x2048.dat.avg} \\ %\cline{2-6}
    & \phongkeythree & \dvtcmdfirstline{simicsphong2896x2896.dat.min} & \dvtcmdfirstline{simicsphong2896x2896.dat.max} & \dvtcmdfirstline{simicsphong2896x2896.dat.std} & \dvtcmdfirstline{simicsphong2896x2896.dat.avg} \\ \hline
  \end{tabular}
  \caption[Benchmark results - software rasterized in Simics]{Benchmarking results whilst software rasterized in the Simics full-system simulator.}
  \label{tab:keyvalsimics}
\end{table*}

% Defines a table outlining the performance of the paper benchmark whilst paravirtualized in the para full-system simulator.
\begin{table*}
  \centering
  \providecommand{\chesskeyone}{$60\times60$ tiles}
  \providecommand{\chesskeytwo}{$84\times84$ tiles}
  \providecommand{\chesskeythree}{$118\times118$ tiles}

  \providecommand{\juliakeyone}{$225$ iterations}
  \providecommand{\juliakeytwo}{$450$ iterations}
  \providecommand{\juliakeythree}{$900$ iterations}

  \providecommand{\phongkeyone}{$1448\times1448$ texels}
  \providecommand{\phongkeytwo}{$2048\times2048$ texels}
  \providecommand{\phongkeythree}{$2896\times2896$ texels}

  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Benchmark} & \multirow{2}{*}{Key} & \multicolumn{4}{p{6cm}|}{\centering Elapsed time (milliseconds)} \\
    \cline{3-6} && \multicolumn{1}{c|}{Min} & \multicolumn{1}{c|}{Max} & \multicolumn{1}{c|}{Std} & \multicolumn{1}{c|}{Avg} \\ \hline
    \multirow{3}{*}{Chess} & \chesskeyone & \dvtcmdfirstline{parachess60x60.dat.min} & \dvtcmdfirstline{parachess60x60.dat.max} & \dvtcmdfirstline{parachess60x60.dat.std} & \dvtcmdfirstline{parachess60x60.dat.avg} \\
    & \chesskeytwo & \dvtcmdfirstline{parachess84x84.dat.min} & \dvtcmdfirstline{parachess84x84.dat.max} & \dvtcmdfirstline{parachess84x84.dat.std} & \dvtcmdfirstline{parachess84x84.dat.avg} \\
    & \chesskeythree & \dvtcmdfirstline{parachess118x118.dat.min} & \dvtcmdfirstline{parachess118x118.dat.max} & \dvtcmdfirstline{parachess118x118.dat.std} & \dvtcmdfirstline{parachess118x118.dat.avg} \\ \hline
    \multirow{3}{*}{Julia} & \juliakeyone & \dvtcmdfirstline{parajulia225.dat.min} & \dvtcmdfirstline{parajulia225.dat.max}	& \dvtcmdfirstline{parajulia225.dat.std} & \dvtcmdfirstline{parajulia225.dat.avg} \\
    & \juliakeytwo & \dvtcmdfirstline{parajulia450.dat.min} & \dvtcmdfirstline{parajulia450.dat.max} & \dvtcmdfirstline{parajulia450.dat.std} & \dvtcmdfirstline{parajulia450.dat.avg} \\
    & \juliakeythree & \dvtcmdfirstline{parajulia900.dat.min} & \dvtcmdfirstline{parajulia900.dat.max} & \dvtcmdfirstline{parajulia900.dat.std} & \dvtcmdfirstline{parajulia900.dat.avg} \\ \hline
    \multirow{3}{*}{Phong} & \phongkeyone & \dvtcmdfirstline{paraphong1448x1448.dat.min} & \dvtcmdfirstline{paraphong1448x1448.dat.max} & \dvtcmdfirstline{paraphong1448x1448.dat.std} & \dvtcmdfirstline{paraphong1448x1448.dat.avg} \\
    & \phongkeytwo & \dvtcmdfirstline{paraphong2048x2048.dat.min} & \dvtcmdfirstline{paraphong2048x2048.dat.max} & \dvtcmdfirstline{paraphong2048x2048.dat.std} & \dvtcmdfirstline{paraphong2048x2048.dat.avg} \\
    & \phongkeythree & \dvtcmdfirstline{paraphong2896x2896.dat.min} & \dvtcmdfirstline{paraphong2896x2896.dat.max} & \dvtcmdfirstline{paraphong2896x2896.dat.std} & \dvtcmdfirstline{paraphong2896x2896.dat.avg} \\ \hline
  \end{tabular}
  \caption[Benchmark results - paravirtualized in Simics]{Benchmarking results whilst paravirtualized in the Simics full-system simulator.}
  \label{tab:keyvalpara}
\end{table*}

% Chess
\paragraph{Chess}
\label{par:results_chess}
From the data visualized in figure \ref{fig:histogramssimicsparachess}, we may observe that the Chess benchmark, when executed in the software rasterized Simics platform, has a relatively broad distribution of its sample density, yet the distribution often seems evenly distributed around a single point.
The right-hand side of the graph, although also showcasing the impaired performance of the corresponding (paravirtualized) platform, visualizes a decrease in the distribution of the sample density.
This is supported by the data presented in table \ref{tab:keyvalpara}.

Based on the data summarized in table \ref{tab:keyvalsimics} (whilst software rasterized in Simics ) and comparing said data to that of table \ref{tab:keyvalpara} (whilst paravirtualized in Simics ), we may observe that the software rasterized solution outperforms it's paravirtualized counterpart; not only in the base experiment, but in all of its input data variations.
The only redeeming attributes the paravirtualized solution brings to the table, as elaborated upon in the above paragraph, is a decrease in the standard deviation of the benchmark profiling.
When comparing these results to the uncompromised hardware accelerated counterpart on the host machine (see figure \ref{fig:histogramshost}), we may observe - albeit considerably less prominent - an adherence to the single-peak behavior in the distribution of the sample density.

The purpose of the Chess benchmark is to locate any bottlenecks related to the number of paravirtualized library invocations (see section \ref{sec:experimentalmethodology_benchmarks}), which was predicted during the pilot study performed for the sake of this experiment.
As such, as presented in section \ref{sec:results_magicinstructionoverhead} in combination with the shaping of the Chess benchmark as presented in section \ref{sec:experimentalmethodology_benchmarks}, there is cause to believe that the prediction of a probable bottleneck in the target - to host communication latency has been confirmed; arguably identifying the weakness of graphics paravirtualization in the Simics full-system simulator.
The conclusions that may be drawn from these further stresses analysis into what is the root cause for the target - to host latency for a multitude of paravirtualized method invocations.
Results related to this matter are presented in section \ref{sec:results_magicinstructionoverhead}. 
Indeed, if proceeding from the findings of section \ref{sec:results_magicinstructionoverhead}, magic instruction overhead accounts for the majority of the elapsed average when paravirtualized in Simics.

\begin{sidewaysfigure*}
  \centering
  \input{gnuhistogramssimicsparachess.tex}
  \caption[Benchmark results - paravirtualized in Simics, Chess]{Histograms depicting benchmark elapsed frametimes in milliseconds and the density distribution of 1000 frames for the Chess benchmark key figure variations whilst software rasterized- and paravirtualized in Simics. The $Y$ axis thus depict sample density. Its axis keys have been removed as they bear no relevance to the outcomes presented in this document.}
  \label{fig:histogramssimicsparachess}
\end{sidewaysfigure*}

% Julia
\paragraph{Julia}
\label{par:results_julia}
In figure \ref{fig:histogramssimicsparajulia}, we may observe double- to triple peak behavior in the distribution of the sample density; both in software rasterized- and paravirtualized platforms.
Albeit the hardware accelerated host profiling (see figure \ref{fig:histogramshost}) may, however minor, suggest such a pattern; it is by all means not significant.
We may observe similar behavior in the distribution of the sample density when profiling the same benchmark whilst paravirtualized in the QEMU -derived Android emulator (see figure \ref{fig:histogramsqemu}).
What causes this behavior is unclear, as frame-to-frame branching in the fractal algorithm is minor and ought not cause such a variance.

The Julia benchmark is incorporated into the experiment to establish how the paravirtualized solution performed under computational stress (see section \ref{sec:experimentalmethodology_benchmarks}).
Using this benchmark, a performance weakness in the software rasterized Simics platform has been identified, with frame times well above the two second mark (see table \ref{tab:keyvalsimics}); with the corresponding maximum frame time in the paravirtualized Simics platform measuring up to to a mere \dvtcmdfirstline{parajulia900.dat.max} ms.
With the Julia benchmark, as visualized in figure \ref{fig:histogramssimicsparajulia}, we have showcased radical performance improvements for computationally intensive graphics kernels and - in turn - identified the capabilities of graphics paravirtualization in the Simics full-system simulator.

\begin{sidewaysfigure*}
  \centering
  \input{gnuhistogramssimicsparajulia.tex}
  \caption[Benchmark results - paravirtualized in Simics, Julia]{Histograms depicting benchmark elapsed frametimes in milliseconds and the density distribution of 1000 frames for the Julia benchmark key figure variations whilst software rasterized- and paravirtualized in Simics. The $Y$ axis thus depict sample density. Its axis keys have been removed as they bear no relevance to the outcomes presented in this document.}
  \label{fig:histogramssimicsparajulia}
\end{sidewaysfigure*}

% Phong
\paragraph{Phong}
\label{par:results_phong}
The Phong benchmark is incorporated into this study for the purposes of analyzing the performance of stressed bandwidth in target -to-host communication; featuring relaying and rendering of a large texture to the simulation host.
The graphs presented in figure \ref{fig:histogramssimicsparaphong} display erratic distribution of the sample density in both software rasterized- and paravirtualized Simics platforms.
Suspicions as to why this is the case are presented in section \ref{sec:threatstovalidity_benchmarkvariations}.

By analyzing the data in presented in tables \ref{tab:keyvalsimics} and \ref{tab:keyvalpara}, it is clear that the Phong benchmark, in terms of average frame times, performs only marginally better or competitively to it's software rasterized equivalent.
However, and mayhaps more interestingly so, executing the benchmark in the paravirtualized Simics environment results in major improvements in terms of frame time maximum and standard deviation; software rasterized samples topping in at \dvtcmdfirstline{simicsphong1448x1448.dat.max} ms where the corresponding paravirtualized maximum is but \dvtcmdfirstline{paraphong1448x1448.dat.max} ms.
Furthermore, one might argue that parts of the distribution align with the double-peak density distribution showcased by the benchmark when hardware accelerated on the simulation host (see figure \ref{fig:histogramshost}).

It is unclear what causes the compiled paravirtualized Phong benchmark to perform only marginally better or competitively to its the software rasterized counterpart.
The effect could be attributed to a bottleneck in the memory page traversal, as described in section \ref{sec:proposedsolutionandimplementation_pagetabletraversal}, signifying a possible weakness in - in addition to the communication latency as established in paragraph \dvtcmdrefname{par:results_chess} - the memory bandwidth of the Simics Pipe (see section \ref{sec:proposedsolutionandimplementation_simicspipe}).

In line with the recorded competitive average- and minimum frame times yet major improvements in frame time maximum and standard deviation; one might also argue that the relatively lightweight benchmark is misrepresentative in that its software rasterized execution is below that of the overhead induced by paravirtualization.
However, due to the deviances described in section \ref{sec:threatstovalidity_benchmarkvariations}, this will not be elaborated upon further for the remainder of this paper.

\begin{sidewaysfigure*}
  \centering
  \input{gnuhistogramssimicsparaphong.tex}
  \caption[Benchmark results - paravirtualized in Simics, Phong]{Histograms depicting benchmark elapsed frametimes in milliseconds and the density distribution of 1000 frames for the Phong benchmark key figure variations whilst software rasterized- and paravirtualized in Simics. The $Y$ axis thus depict sample density. Its axis keys have been removed as they bear no relevance to the outcomes presented in this document.}
  \label{fig:histogramssimicsparaphong}
\end{sidewaysfigure*}

% Magic Instruction Overhead
\subsubsection{Magic Instruction Overhead}
\label{sec:results_magicinstructionoverhead}
In Simics, magic instructions incur a context switch when exiting the simulation and beginning execution in the real world.
This affects the performance by forcing the simulation to no longer be executed in native mode; inhibiting the simulatory performance improvements given by hardware-assisted virtualization.
Additionally, this also entail Simics no longer being able to utilize just-in-time compilation to speed up execution; having to rely on regular code interpretation.
As such, in great numbers, magic instructions may potentially affect performance. 

\begin{figure*}
  \centering
  \begin{lstlisting}
    int sercon = open("/dev/ttyS0",O_RDWR|O_NOCTTY);
  \end{lstlisting}
  \begin{minipage}{.5\textwidth}
    \centering
    \lstinputlisting{pseudoindividual.txt}
    \caption[Pseudocode - magic instruction profiling, per-invocation]{Profiling of magic instructions on a per-invocation basis.}
    \label{fig:magicinstructionpseudocodeforeach}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \lstinputlisting{pseudobatch.txt}
    \caption[Pseudocode - magic instruction profiling, per-batch]{Profiling of magic instructions in batches.}
    \label{fig:magicinstructionpseudocodeforall}
  \end{minipage}
\end{figure*}


In line with the effect that this overhead may have on software such as benchmark Chess, two tests have been performed to establish the overhead magic instructions may induce.
These tests have been run in two instances; one in which each magic instruction invocation is profiled separately using the methodology described in section \ref{sec:threatstovalidity_platformprofiling}, and another in which batches of magic instruction invocations are measured to reduce influence of profiling overhead (see figures \ref{fig:magicinstructionpseudocodeforeach} and \ref{fig:magicinstructionpseudocodeforall}).
This is due to profiling overhead, described in section \ref{sec:threatstovalidity_platformprofiling}, having influenced the original profilation results too greatly.
As such, magic instructions has to be batched in order to be profiled correctly.
Below, the results of both of these tests are presented.

\paragraph{Individual profiling}
\label{par:results_magicinstructionoverhead_individualprofiling}
The per-invocation profiling of $1000$ magic instructions is presented in figure \ref{fig:histogrammagicinstructionsforeach} as a histogram.
All samples compiled in said test have been subtracted with the established average performance, as described in section \ref{sec:threatstovalidity_platformprofiling}.
In line with this modification of the source data, due to the established profiling average overhead of \dvtcmdfirstline{profile.dat.avg} having relatively much influence on the data, the visualization features only ten histogram bins - in coagency with the approximate data.
As such, any offset values below that of zero are placed in the first bin.
Furthermore, any values outside that of the standard deviation are not visualized in this figure.

\begin{figure}
  \centering
  \input{gnuhistogrammagicinstructionsforeach.tex}
  \caption[Per-invocation magic instruction profilation histogram]{Histogram depicting approximative per-invocation profilation and percentage of density distribution of 1000 magic instructions in milliseconds.}
  \label{fig:histogrammagicinstructionsforeach}
\end{figure}

From this data, it is apparent that roughly $45\%$ of profiled samples fall in the interval of $0$-$0.5$ milliseconds, a relatively high measurement compared to that of the batch profiling (see paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_batchprofiling}).
The figure is complemented with the analyzed data presented in table \ref{tab:magicinstructionsforeach}

\begin{table}
  \centering
  \begin{tabular}{llll}
    Min & Max & Std & Avg \\ \hline
    \dvtcmdfirstline{magicinstrprofileeach.dat.min} & \dvtcmdfirstline{magicinstrprofileeach.dat.max} & \dvtcmdfirstline{magicinstrprofileeach.dat.std} & \dvtcmdfirstline{magicinstrprofileeach.dat.avg} \\
  \end{tabular}
  \caption[Magic instruction profiling tabular, per-invocation]{Profiling results for individual magic instructions. Presented in milliseconds and modified in accordance to the profiling overhead established in section \ref{sec:threatstovalidity_platformprofiling}.}
  \label{tab:magicinstructionsforeach}
\end{table}

\paragraph{Batch profiling}
\label{par:results_magicinstructionoverhead_batchprofiling}
In order to complement the data presented in paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_individualprofiling}, corresponding measurements for batch magic instruction invocations is collected in accordance to the pseudo code presented in table \ref{tab:magicinstructionsforall}.
In this way, we wanted to circumvent any obscurities induced by profiling overhead (see section \ref{sec:threatstovalidity_platformprofiling}).

\begin{table}
  \centering
  \begin{tabular}{llll}
    Min & Max & Std & Avg \\ \hline
    \dvtcmdfirstline{magicinstrprofileall.dat.min} & \dvtcmdfirstline{magicinstrprofileall.dat.max} & \dvtcmdfirstline{magicinstrprofileall.dat.std} & \dvtcmdfirstline{magicinstrprofileall.dat.avg} \\
  \end{tabular}
  \caption[Magic instruction profiling tabular, per-batch]{Profiling results for batched magic instructions. Presented in milliseconds and modified in accordance to the profiling overhead established in section \ref{sec:threatstovalidity_platformprofiling}.}
  \label{tab:magicinstructionsforall}
\end{table}

From the data presented in paragraphs \dvtcmdrefname{par:results_magicinstructionoverhead_individualprofiling} and \dvtcmdrefname{par:results_magicinstructionoverhead_batchprofiling}, we may deduce that the profiling overhead induced when measuring the elapsed time of individual magic instructions causes misrepresentation in the actual elapsed time of said magic instructions.
This is clear when analyzing the batch-wise data presented in table \ref{tab:magicinstructionsforall}, where the cost of a magic instruction invocation is significantly lower.
This may be caused by the system call \dvtcmdcodeinline{tcdrain}, which	waits until all output written to the referenced serial console has been transmitted.
One may speculate that said system call may be the cause for eccentric behavior in the profiling.

As such, the per-invocation profiling elaborated upon in paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_individualprofiling} is rendered less reliable due to the eccentric profiled behavior.
From this we may conclude that the execution of $1000$ magic instructions is expected to induce an average overhead of roughly \dvtcmdfirstline{magicinstrprofileall.dat.avg} ms, accounting for profiling errors as presented in section \ref{sec:threatstovalidity_platformprofiling}\footnote{Note that this may be regarded as an optimum case, since batch execution of magic instructions may induce an executional pattern which could lead to improved results. However, this is just speculation.}.
Due to the volatility of the profiling presented in paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_individualprofiling}, the author will refrain from using these results for any definite conclusions.

Thus, the established overhead cost for $1000$ magic instructions is summarized in table \ref{tab:magicinstructionsforall}.

% Platform Comparison
\subsubsection{Platform Comparison}
\label{sec:analysisexperiment_platformcomparison}
In the sections above, results have been presented indicating performance gains, and potential for gains, in Simics platforms by the means of accelerating graphics using paravirtualization.
However, in accordance to figure \ref{fig:histogramsqemu} and table \ref{tab:keyvalqemu}, the platform on which the these results have been produced also utilizing paravirtualized methodology, there is much potential for improvement.
The benchmarks, when executed in the paravirtualized Android emulator, exhibit better performance in each test performed for the purposes of this paper; most notably outperforming the software rasterized Simics platform for the Chess benchmark.
The Chess benchmark, incurring such an overhead when paravirtualized in Simics (see paragraph \dvtcmdrefname{par:results_chess}) due to overhead induced by magic instructions (see section \ref{sec:results_magicinstructionoverhead}), performs but roughly two times worse than its hardware accelerated counterpart at \dvtcmdfirstline{qemuchess84x84.dat.avg} milliseconds average (see tables \ref{tab:keyvalqemu} and \ref{tab:keyvalhost}).
These results indicate potential for improvement in the target -to-host communications in Simics.
In fact, considering the large similarities in the paravirtualized methodology for graphics acceleration these two platforms share, the reference performance of the QEMU -derived Android emulator may be considered a goal for a potential productification of paravirtualized graphics in the Simics full-system simulator.

These comparisons suggest that the recorded performance for the benchmarks, devised for the purpose of this study, is not necessarily representative for paravirtualized graphics in general.
Furthermore, the comparison to the QEMU -derived Android emulator indicates that the shaping of the magic instruction -utilizing Simics Pipe - as established being the bottleneck during execution of the paravirtualized Chess benchmark (see section \ref{sec:results_magicinstructionoverhead}) - may have potential of improvement of approximately one order of magnitude (see tables \ref{tab:keyvalqemu} and \ref{tab:keyvalpara}).
