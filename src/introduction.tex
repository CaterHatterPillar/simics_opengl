% introduction.tex

% Introduction
\section{Introduction}
\label{sec:introduction}
Virtual platforms are becoming an important tool in the software industry in order to provide cost-effective time-to-market gains and meet the ever-shortening product life-cycles~\dvtcmdcitebib[p.~50,~52]{journals:magnusson:2002}\dvtcmdcitebib[p.~268]{journals:yi:2006}.
Virtual platforms deliver these time-to-market benefits in two major ways.
First of all, virtual platforms enable pre-silicon development, that is; software development can begin prior to next-generation hardware being available~\dvtcmdcitebib[p.~52]{journals:magnusson:2002}.
Secondly, virtual platforms may provide additional development tools compared to working with actual hardware.
For example, some virtual platforms allow simulated systems, often known as simulation targets, to be stopped synchronously without affecting timing or states of the target software~\dvtcmdcitebib[p.~61]{inproceedings:yu:2012}, and allow investigation of race conditions and other parallel programming issues~\dvtcmdcitebib[p.~1]{inproceedings:schumacher:2010}\dvtcmdcitebib[p.~7]{publications:leupers:2010}.
Additionally, such platforms may allow intricate inspection of simulated hardware, such as memory, caches, and registers~\dvtcmdcitebib[p.~54]{journals:magnusson:2002}.
Some virtual platforms provide advanced features such as reverse execution (the ability to run a simulation backwards) and checkpointing (functionality to save- and restore the state of a simulation).
These features are useful for debugging and testing a diverse range of software; from firmware to end-user applications~\dvtcmdcitebib[p.~25]{publications:leupers:2010}.

There are several techniques to provide fast functional virtual platforms that are running CPU workloads.
Typical methods include interpretation~\dvtcmdcitebib[p.~35]{journals:smith:2005}, just-in-time compilation~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013}, and hardware-assisted virtualization~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013}\dvtcmdcitebib[p.~38,~39]{publications:leupers:2010}.
Virtual platforms using these techniques can typically achieve a simulation performance in the range of $10$-$1000$ MIPS~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013} up to $5000$ MIPS~\dvtcmdcitebib[p.~38,~39]{publications:leupers:2010}.

The GPU is a vital part in delivering good user experiences on many devices ranging from wearable, hand held, and portable units to desktop computers.
Since GPUs operate significantly different from CPUs, utilizing massively parallelized instruction sets to increase throughput, they continue to pose unique challenges to designers and developers~\dvtcmdcitebib[ch.~13]{publications:kirk:2010}.
The increased utilization of GPUs for general purpose workloads has extended the need for virtualization of such hardware in situations when hardware is busy, unavailable, not sufficient, or for the purposes of debugging and profiling~\dvtcmdciteref{web:microsoft:2013:warp}.
For example, developers interested in benchmarking or driver development for next generation GPU- or CPUs may require detailed simulators that provide insight into execution engines and pipelines~\dvtcmdcitebib[p.~1]{inproceedings:schumacher:2010}.
Albeit applicable in certain use-cases and capable of running 'toy' applications, such platforms are often orders of magnitude too slow to run commercial workloads~\dvtcmdcitebib[p.~50]{journals:magnusson:2002}.
Application developers, on the other hand, do not necessarily care for the internal workings of hardware as they typically work at a higher abstraction level, for example; utilizing a graphics API that, in turn, communicate with the device driver.
As such, application developers may be more interested in achieving decent simulation performance rather than a timing-accurate processor model (see \dvtcmdcitebib[p.~30]{publications:leupers:2010} for an analysis of compromises in system simulation).
However, due to large differences between CPU and GPU architecture, simply delegating GPU-bound workloads to the CPU is rarely feasible in terms of performance.

Common approaches to accelerate simulation performance include creating a functionally accurate model of the GPU, where internal details may be simplified, or using software rasterization without involving the GPU model.
However, these methodologies traditionally incur heavy performance losses in comparison to hardware acceleration.
In order to achieve better performance, one may 'offload' such kernels to the GPU of the system on which the simulation runs (often referred to as the simulation host).
There are a number of ways to do so, such as relying on PCI~passthrough and similar technologies to grant access to the underlying host hardware from within the virtual platform~\dvtcmdcitebib[p.~415,~416]{inproceedings:regola:2010}\dvtcmdciteref{web:jones:2009}, or utilizing a concept commonly referred to as 'Paravirtualization' at a higher level of abstraction (e.g., the graphics library).
Paravirtualization is defined as selectively modifying the virtual architecture to enhance scalability, performance, or simplicity~\dvtcmdcitebib[p.~165-166]{magazines:bartholomew:2006}.
Effectively, this entails modifying the virtual machine to be similar, but not identical, to host hardware~\dvtcmdcitebib[p.~165]{journals:barham:2003}.
As such, one may simplify the virtualization process by neglecting some hardware compatibility~\dvtcmdcitebib[p.~1]{inproceedings:youseff:2006}.

This paper comprises an investigation of OpenGL graphics paravirtualization in the Simics full-system simulator.
The work presents an implementation of accelerated OpenGL~ES~$2.0$ graphics by the means of paravirtualization; using magic instructions as a communications bridge between target and host systems.
Additionally, we present performance benchmarks stressing important attributes of the devised solution.
We then evaluate the performance of common software rasterization in comparison to the accelerated platform.
Furthermore, our study identifies performance bottlenecks that may obstruct paravirtualized real-time graphics.
The results presented in this paper show performance improvements of up to $34$ times compared to software rasterized counterparts.

The remainder of the paper present previous research and the objectives of the study, followed by a descriptory chapter of the devised paravirtualized implementation and the performed experiment, and concluded by a section summarizing our findings.
