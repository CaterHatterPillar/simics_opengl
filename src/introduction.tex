% introduction.tex

% Introduction
\section{Introduction}
\label{sec:introduction}
Virtual platforms are becoming an important tool in the software industry in order to provide cost-effective time-to-market gains and meet the ever-shortening product life-cycles~\dvtcmdcitebib[p.~50,~52]{journals:magnusson:2002}\dvtcmdcitebib[p.~268]{journals:yi:2006} (see \dvtcmdcitebib{magazines:magnusson:2004} for an overview of the benefits of full-system simulation).
Virtual platforms deliver these time-to-market -benefits in two major ways.
Virtual platforms enable pre-silicon development, that is; software development can begin prior to next-generation hardware being available~\dvtcmdcitebib[p.~52]{journals:magnusson:2002}.
Furthermore, virtual platforms may provide additional development tools compared to working with actual hardware.
For example, some virtual platforms allow simulated systems, often known as simulation targets, to be stopped synchronously without affecting timing or states of the target software~\dvtcmdcitebib[p.~61]{inproceedings:yu:2012}, and allow investigation of race conditions and other parallel programming issues~\dvtcmdcitebib[p.~1]{inproceedings:schumacher:2010}\dvtcmdcitebib[p.~7]{publications:leupers:2010}.
Additionally, such platforms may allow intricate inspection of simulated hardware, such as memory, caches, and registers~\dvtcmdcitebib[p.~54]{journals:magnusson:2002}.
Some virtual platforms provide advanced features such as reverse execution ~\dvtcmdcitebib[p.~30,~31]{publications:leupers:2010} (the ability to run a simulation backwards) and checkpointing ~\dvtcmdcitebib[p.~28,~29]{publications:leupers:2010} (functionality to save- and restore the state of a simulation).
These features are useful for debugging and testing a diverse range of software; from firmware to end-user applications~\dvtcmdcitebib[p.~25]{publications:leupers:2010}.

There are several techniques to provide fast functional virtual platforms that are running CPU-bound workloads.
Typical methods include interpretation ~\dvtcmdcitebib[p.~35]{journals:smith:2005}, just-in-time compilation~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013}, and Hardware-assisted Virtualization ~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013}\dvtcmdcitebib[p.~38,~39]{publications:leupers:2010}.
Virtual platforms using these techniques can typically achieve a simulation performance in the range of $10$-$1000$ MIPS~\dvtcmdcitebib[p.~24,~25]{journals:aarno:2013}, but there is recorded performance of up to $5000$ MIPS~\dvtcmdcitebib[p.~38,~39]{publications:leupers:2010}.

The GPU has become a vital part in delivering good user experience on many types of devices ranging from wearable, hand held, and portable units to desktop computers.
In contrast to CPU-bound workloads, when developing GPU kernels, seemingly insignificant additions to code may cause significant changes in performance due to massively parallelized instruction sets (see Performance Considerations by Kirk and Hwu~\dvtcmdcitebib[ch.~6]{publications:kirk:2010} for an analysis on the volatility of GPU performance).
Since GPUs operate significantly different from CPUs, they pose unique challenges to designers and developers (see Parallel Programming and Computational Thinking by Kirk and Hwu~\dvtcmdcitebib[ch.~13]{publications:kirk:2010} for an elaboration on GPGPU methodology).
The increased utilization of GPUs for general purpose workloads has extended the need for virtualization of such hardware in situations when hardware is busy, unavailable, not sufficient, or for the purposes of debugging and profiling (an example of modern technology developed for said purposes is Microsoft~WARP - Microsofts latest addition to the DirectX framework~\dvtcmdciteref{web:microsoft:2013:warp}).
Yet seemingly few virtual platforms support virtualization of GPUs, despite their influence on modern computing.

For the purposes of simulation, different solutions follow varying use-cases. For example, developers interested in benchmarking or driver development for next generation GPU- or CPUs may require detailed simulators that provide insight into execution engines and pipelines~\dvtcmdcitebib[p.~1]{inproceedings:schumacher:2010}.
Albeit applicable in certain use-cases and capable of running 'toy' applications, such platforms are often orders of magnitude too slow to run commercial workloads~\dvtcmdcitebib[p.~50]{journals:magnusson:2002}.
Application developers, on the other hand, do not necessarily care for the internal workings of hardware as they typically work at a higher abstraction level, for example; utilizing a graphics API (such as OpenGL) that, in turn, communicate with the device driver.
As such, application developers may be more interested in achieving decent simulation performance rather than a timing-accurate processor model (see \dvtcmdcitebib[p.~30]{publications:leupers:2010} for an analysis of compromises in system simulation).
However, due to large differences in-between CPU- and GPU architecture, simply delegating commonly GPU-bound workloads to the CPU is rarely feasible in terms of performance.

Common approaches to achieve better simulation performance includes creating a functionally accurate model of the GPU (inducing the advantage of being able to utilize the same software (drivers, libraries, etc.) as the simulated platform), where internal details may be simplified, or using software rasterization without involving the GPU model.
However, these methodologies traditionally incur heavy performance losses in comparison to GPU hardware acceleration.
In order to achieve better performance, one may offload such kernels to the GPU of the system on which the simulation runs, often referred to as the simulation host.
There are a number of way to achieve such acceleration, such as relying on PCI~passthrough and similar technologies to grant access to the underlying host hardware from within the virtual platform~\dvtcmdcitebib[p.~415,~416]{inproceedings:regola:2010} (see \dvtcmdciteref{web:jones:2009} for an overview on PCI~passthrough), or utilizing a concept commonly referred to as 'Paravirtualization' at a higher level of abstraction (e.g., the OpenGL library).

Paravirtualization is a relatively new term originally defined as selectively modifying the virtual architecture to enhance scalability, performance, or simplicity~\dvtcmdcitebib[p.~165-166]{magazines:bartholomew:2006}.
Effectively, this means modifying the virtual machine to be similar, but not identical, to host hardware~\dvtcmdcitebib[p.~165]{journals:barham:2003}.
As such, one may simplify the virtualization process by neglecting some hardware compatibility~\dvtcmdcitebib[p.~1]{inproceedings:youseff:2006}.
Implementing GPU simulation by the means of Paravirtualization provides the benefits of improved simulation performance, albeit it may entail loosing some of the benefits a virtual platform can provide.
For example, it may be challenging to support advanced features such as deterministic execution, checkpointing, and reverse execution.
Additionally, paravirtualization entails modifying the simulated system, since some part of the target machine must be modified to accommodate the paravirtualized methodology.
\hl{Yet, aforementioned technique is the preferred method of Google in the Android SDK when utilizing the QEMU virtual platform in order to simulate OpenGL~ES.}

This dissertation comprises an investigation of Paravirtualization of OpenGL~ES in the Simics virtual platform.
The work presents an implementation of accelerated OpenGL~ES~$2.0$ graphics by the means of Paravirtualization and using magic instructions as a communications bridge.
In addition to this, we present the development of three benchmarks stressing important attributes of the devised solution.
Using the results collected from the execution of said benchmarks on a number of platforms, the solution is evaluated.
Furthermore, this dissertation identifies performance bottlenecks obstructing large numbers of paravirtualized invocations for the purposes of real-time graphics.
The results presented in this thesis show performance improvements of up to $34$ times in relation to software rasterized counterparts.

The remainder of the report presents the objectives and question formulations adhering to the presented study.
Additionally, an introductory chapter on relevant background and preceding work in the area - expanded upon by a succeeding chapter concerning the Simics full-system simulator - means to bring about many of the terms and concepts used in the subsequent material.
Next, a description of the devised paravirtualized implementation is presented along with a chapter on experimental methodology.
Furthermore, this thesis features an elaboration on potential threats to validity before detailing the results collected using the methods described in prior chapters.
The material is then finalized by a speculative discussion chapter, analyzing the paravirtualized solution in regard to advanced functionality in the Simics full-system simulator, foregoing the conclusions which may be established from the compiled material.
Finally, the dissertation is concluded by a report on recommended future work in the area.
